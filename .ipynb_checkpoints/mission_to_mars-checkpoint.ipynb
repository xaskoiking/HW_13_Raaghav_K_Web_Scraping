{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Dependencies\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declaring variables\n",
    "\n",
    "\n",
    "#1.0) URL to fetch the Latest News Title and Paragraph Text\n",
    "mars_news = \"https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest\"\n",
    "\n",
    "#2.0) URL to fetch the MARS image\n",
    "mars_image = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "\n",
    "#3.0) URL to get twitter details\n",
    "twitter_details = \"https://twitter.com/marswxreport?lang=en\"\n",
    "\n",
    "#4.0) URL to get MARS facts\n",
    "mars_facts = \"https://space-facts.com/mars/\"\n",
    "\n",
    "#5.0) URL to get MARS Hemisphere\n",
    "mars_hemisphere = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to call the chrome driver\n",
    "def init_browser():\n",
    "    # @NOTE: Replace the path with your actual path to the chromedriver\n",
    "    executable_path = {\"executable_path\": \"C:/chromedriver_win32/chromedriver.exe\"}\n",
    "    return Browser(\"chrome\", **executable_path, headless=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the browswer\n",
    "browser = init_browser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> MARS News Title and Paragraph </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-191-0f83bf5cf349>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m#Storing the news Para Text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mnewsParaText\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnewsParaText\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "#Scraping the URL\n",
    "browser.visit(mars_news)\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "#Finding the latest News Title Header\n",
    "newsTitleHeader = soup.find('div', attrs={'class':'content_title'})\n",
    "newsParaText = soup.find('div', attrs={'class':'article_teaser_body'})\n",
    "print(newsParaText)\n",
    "\n",
    "#Grabbing the news title\n",
    "for a in newsTitleHeader.find_all('a'):\n",
    "    newsTitle = a.text #for getting text between the link\n",
    "\n",
    "#Storing the news Para Text\n",
    "newsParaText = newsParaText.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "browser.visit(mars_news)\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "newsDetails = soup.findAll('ul', attrs={'class': 'item_list'})\n",
    "for articlesList in newsDetails:\n",
    "    for indArticle in articlesList:\n",
    "        if counter == 0:\n",
    "            news_date = indArticle.find('div', attrs={'class': 'list_date'}).text\n",
    "            news_title = indArticle.find('div', attrs={'class': 'content_title'}).text\n",
    "            news_p = indArticle.find('div', attrs={'class': 'article_teaser_body'}).text\n",
    "            counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsParaText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- SUPPORTING CODES ------\n",
    "#for a in price_box.find_all('a'):\n",
    "#    print(a.get('href')) #for getting link\n",
    "#    print(a.text) #for getting text between the link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> MARS Image </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping the URL\n",
    "browser.visit(mars_image)\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "#Finding the latest News Title Header\n",
    "featureImage = soup.find('article', attrs={'class':'carousel_item'})\n",
    "\n",
    "featuredImageBuffer = featureImage.attrs['style']\n",
    "featured_image_url  = \"https://www.jpl.nasa.gov\" + featuredImageBuffer[featuredImageBuffer.find('url') + 5: len(featuredImageBuffer) - 3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Twitter Details on temperature </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is to get the latest tweet\n",
    "counter = 0\n",
    "\n",
    "#Scraping the URL\n",
    "browser.visit(twitter_details)\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "#Finding all the tweets content\n",
    "allTweetsContainter = soup.findAll('div', attrs={'class': 'js-tweet-text-container'})\n",
    "\n",
    "#Iterating the tweets to get the temperature details\n",
    "for allTweetsContainter in allTweetsContainter:\n",
    "    if(\"Sol\" in allTweetsContainter.find('p').text and \"pressure\" in allTweetsContainter.find('p').text and \"daylight\" in allTweetsContainter.find('p').text and counter == 0):\n",
    "        mars_weather = allTweetsContainter.find('p').text\n",
    "        counter = counter + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sol 2166 (2018-09-09), high -10C/14F, low -72C/-97F, pressure at 8.87 hPa, daylight 05:40-17:56'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mars_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Mars Facts </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is to get the Mars Facts\n",
    "counter = 0\n",
    "import pandas as pd\n",
    "\n",
    "#Scraping the URL\n",
    "browser.visit(mars_facts)\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "#Finding all the facts\n",
    "marsFacts = soup.findAll('table')[0]\n",
    "\n",
    "\n",
    "tables = pd.read_html(mars_facts)\n",
    "tables\n",
    "\n",
    "df = tables[0]\n",
    "df.columns = ['desc', 'facts']\n",
    "\n",
    "desc = df[\"desc\"].values.tolist()\n",
    "facts = df[\"facts\"].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Mars Hemisphere </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping the URL\n",
    "browser.visit(mars_hemisphere)\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "#marsHemisphere\n",
    "marsHemisphere = soup.findAll('div', attrs={'class': 'item'})\n",
    "\n",
    "#Adding the text to the lit, so that you can use the linktext\n",
    "imageTextList = []\n",
    "\n",
    "#Iterating the class -- item and storing the tex\n",
    "for a in marsHemisphere:\n",
    "    for b in a.findAll('a'):\n",
    "        if(len(b.text) > 0):\n",
    "            imageTextList.append(b.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is use to launch the home page\n",
    "browser.visit(mars_hemisphere)\n",
    "html = browser.html\n",
    "browser.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemisphere_image_urls = []\n",
    "{\"title\": \"Valles Marineris Hemisphere\", \"img_url\": \"...\"},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in imageTextList:\n",
    "    #Parent Page\n",
    "    browser.visit(mars_hemisphere)\n",
    "    html = browser.html\n",
    "    \n",
    "    #Click Action\n",
    "    browser.click_link_by_partial_text(text)\n",
    "    \n",
    "    #Child Page\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    image = soup.findAll('img')\n",
    "    for a in image:\n",
    "        if('full' in a.get('src')):\n",
    "        dictDetails = {title:text}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://astrogeology.usgs.gov/search/map/Mars/Viking/cerberus_enhanced'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### TEST ###############\n",
    "# a) need to iterate the imageTextList and then call the link for all\n",
    "\n",
    "\n",
    "#Calling the click ( from the looping)\n",
    "browser.click_link_by_partial_text('Cerberus Hemisphere Enhanced')\n",
    "#Once the clikc in enabled, we pull the next page details\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "image = soup.findAll('img')\n",
    "#iterate the image details and the get the one with full sized.. you need to append \"https://astrogeology.usgs.gov\" before it\n",
    "for a in image:\n",
    "    if('full' in a.get('src')):\n",
    "        print(a.get('src'))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cache/images/cfa62af2557222a02478f1fcd781d445_cerberus_enhanced.tif_full.jpg\n"
     ]
    }
   ],
   "source": [
    "image = soup.findAll('img')\n",
    "for a in image:\n",
    "    if('full' in a.get('src')):\n",
    "        print(a.get('src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData]",
   "language": "python",
   "name": "conda-env-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
